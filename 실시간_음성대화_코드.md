# ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™” ì‹œìŠ¤í…œ ì „ì²´ ì½”ë“œ

## ğŸ“‹ ëª©ì°¨
1. [í”„ë¡ íŠ¸ì—”ë“œ ì»´í¬ë„ŒíŠ¸](#í”„ë¡ íŠ¸ì—”ë“œ-ì»´í¬ë„ŒíŠ¸)
2. [API ë¼ìš°íŠ¸](#api-ë¼ìš°íŠ¸)
3. [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)
4. [íŒ¨í‚¤ì§€ ì„¤ì •](#íŒ¨í‚¤ì§€-ì„¤ì •)

---

## í”„ë¡ íŠ¸ì—”ë“œ ì»´í¬ë„ŒíŠ¸

### `src/components/StreamingVoiceChat.tsx`
```tsx
'use client'

import React, { useState, useRef, useEffect } from 'react'
import { Mic, MicOff, Volume2, VolumeX, Play, Pause } from 'lucide-react'

interface StreamingVoiceChatProps {}

interface Message {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  isStreaming?: boolean
}

export const StreamingVoiceChat: React.FC<StreamingVoiceChatProps> = () => {
  const [isListening, setIsListening] = useState(false)
  const [isProcessing, setIsProcessing] = useState(false)
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [conversation, setConversation] = useState<Message[]>([])
  const [currentTranscript, setCurrentTranscript] = useState('')
  const [streamingResponse, setStreamingResponse] = useState('')
  const [selectedLanguage, setSelectedLanguage] = useState('ko-KR')
  const [isWhale, setIsWhale] = useState(false)
  const [useWhisperSTT, setUseWhisperSTT] = useState(false)

  const recognitionRef = useRef<any>(null)
  const synthRef = useRef<SpeechSynthesis | null>(null)
  const streamingTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const currentAudioRef = useRef<HTMLAudioElement | null>(null)

  // ë¸Œë¼ìš°ì € ê°ì§€ í•¨ìˆ˜
  const detectBrowser = () => {
    if (typeof window === 'undefined') return false
    
    const userAgent = navigator.userAgent.toLowerCase()
    const isWhaleBrowser = userAgent.includes('whale')
    
    console.log('ğŸ‹ ë¸Œë¼ìš°ì € ê°ì§€:', {
      userAgent: navigator.userAgent,
      isWhale: isWhaleBrowser,
      webSpeechSupport: !!(window as any).SpeechRecognition || !!(window as any).webkitSpeechRecognition
    })
    
    return isWhaleBrowser
  }

  // ë¸Œë¼ìš°ì € ê°ì§€ ë° STT ë°©ì‹ ê²°ì •
  useEffect(() => {
    if (typeof window !== 'undefined') {
      const whaleBrowser = detectBrowser()
      setIsWhale(whaleBrowser)
      setUseWhisperSTT(whaleBrowser) // Whale ë¸Œë¼ìš°ì €ë©´ ìë™ìœ¼ë¡œ Whisper STT ì‚¬ìš©
      
      console.log('ğŸŒ ë¸Œë¼ìš°ì € í™˜ê²½ í™•ì¸:', {
        userAgent: navigator.userAgent,
        language: navigator.language,
        languages: navigator.languages,
        onLine: navigator.onLine,
        protocol: window.location.protocol,
        isWhale: whaleBrowser,
        useWhisperSTT: whaleBrowser
      })
    }
  }, [])

  // Web Speech API ì„¤ì • (Whaleì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
  useEffect(() => {
    if (typeof window !== 'undefined' && !useWhisperSTT) {
      const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
      
      console.log('ğŸ™ï¸ Speech Recognition ì§€ì›:', {
        SpeechRecognition: !!SpeechRecognition,
        webkitSpeechRecognition: !!(window as any).webkitSpeechRecognition,
        SpeechRecognitionNative: !!(window as any).SpeechRecognition
      })
      
      if (SpeechRecognition) {
        const recognition = new SpeechRecognition()
        
        // ê¸°ë³¸ ì„¤ì •ë§Œ ì‚¬ìš©
        recognition.continuous = false  // ì¼ë‹¨ falseë¡œ í…ŒìŠ¤íŠ¸
        recognition.interimResults = true
        recognition.lang = selectedLanguage
        recognition.maxAlternatives = 1  // 1ë¡œ ì¤„ì—¬ì„œ í…ŒìŠ¤íŠ¸
        
        console.log('ğŸ”§ ìŒì„± ì¸ì‹ ì„¤ì •:', {
          continuous: recognition.continuous,
          interimResults: recognition.interimResults,
          lang: recognition.lang,
          maxAlternatives: recognition.maxAlternatives
        })

        recognition.onstart = () => {
          console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘')
          setIsListening(true)
        }

        recognition.onresult = (event: any) => {
          console.log('ğŸ“ ìŒì„± ì¸ì‹ ê²°ê³¼ ì´ë²¤íŠ¸:', {
            resultIndex: event.resultIndex,
            resultsLength: event.results.length,
            results: Array.from(event.results).map((result: any, index: number) => ({
              index,
              transcript: result[0].transcript,
              confidence: result[0].confidence,
              isFinal: result.isFinal
            }))
          })

          let interimTranscript = ''
          let finalTranscript = ''

          for (let i = event.resultIndex; i < event.results.length; i++) {
            const result = event.results[i]
            
            // ëª¨ë“  ëŒ€ì•ˆ í™•ì¸
            let bestTranscript = ''
            let bestConfidence = 0
            
            for (let j = 0; j < result.length; j++) {
              const alternative = result[j]
              console.log(`ğŸ“ ê²°ê³¼ ${i}-${j}:`, {
                transcript: alternative.transcript,
                confidence: alternative.confidence,
                isFinal: result.isFinal
              })
              
              if (alternative.confidence > bestConfidence || j === 0) {
                bestTranscript = alternative.transcript
                bestConfidence = alternative.confidence
              }
            }
            
            if (bestTranscript.trim()) { // ë¹ˆ ë¬¸ìì—´ ì²´í¬
              if (result.isFinal) {
                finalTranscript += bestTranscript
              } else {
                interimTranscript += bestTranscript
              }
            }
          }

          if (interimTranscript) {
            console.log('ğŸ”„ ì¤‘ê°„ ê²°ê³¼:', interimTranscript)
            setCurrentTranscript(interimTranscript)
          }

          if (finalTranscript) {
            console.log('âœ… ìµœì¢… ìŒì„± ì¸ì‹:', finalTranscript)
            handleUserSpeech(finalTranscript)
            setCurrentTranscript('')
          }
        }

        recognition.onerror = (event: any) => {
          console.error('âŒ ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', {
            error: event.error,
            message: event.message,
            type: event.type
          })
          setIsListening(false)
        }

        recognition.onend = () => {
          console.log('ğŸ”‡ ìŒì„± ì¸ì‹ ì¢…ë£Œ')
          setIsListening(false)
          
          // continuousê°€ falseì¼ ë•Œ ìë™ ì¬ì‹œì‘
          if (!recognition.continuous && isListening) {
            console.log('ğŸ”„ ìŒì„± ì¸ì‹ ìë™ ì¬ì‹œì‘')
            setTimeout(() => {
              if (recognitionRef.current && !isProcessing && !isSpeaking) {
                try {
                  recognitionRef.current.start()
                } catch (error) {
                  console.error('âŒ ì¬ì‹œì‘ ì‹¤íŒ¨:', error)
                }
              }
            }, 100)
          }
        }

        recognition.onaudiostart = () => {
          console.log('ğŸµ ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘')
        }

        recognition.onaudioend = () => {
          console.log('ğŸµ ì˜¤ë””ì˜¤ ìº¡ì²˜ ì¢…ë£Œ')
        }

        recognition.onsoundstart = () => {
          console.log('ğŸ”Š ì†Œë¦¬ ê°ì§€ ì‹œì‘')
        }

        recognition.onsoundend = () => {
          console.log('ğŸ”Š ì†Œë¦¬ ê°ì§€ ì¢…ë£Œ')
        }

        recognition.onspeechstart = () => {
          console.log('ğŸ—£ï¸ ìŒì„± ê°ì§€ ì‹œì‘')
        }

        recognition.onspeechend = () => {
          console.log('ğŸ—£ï¸ ìŒì„± ê°ì§€ ì¢…ë£Œ')
        }

        recognition.onnomatch = () => {
          console.log('â“ ìŒì„± ì¸ì‹ ê²°ê³¼ ì—†ìŒ')
        }

        recognitionRef.current = recognition
      }

      synthRef.current = window.speechSynthesis
    }
  }, [selectedLanguage, useWhisperSTT])

  // Whisper STTë¥¼ ìœ„í•œ MediaRecorder ì„¤ì •
  const setupMediaRecorder = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        } 
      })
      
      // ì§€ì›ë˜ëŠ” MIME íƒ€ì… í™•ì¸ í›„ ì„¤ì •
      let mimeType = 'audio/webm;codecs=opus'
      if (MediaRecorder.isTypeSupported('audio/wav')) {
        mimeType = 'audio/wav'
      } else if (MediaRecorder.isTypeSupported('audio/webm')) {
        mimeType = 'audio/webm'
      }
      
      console.log('ğŸ¤ ì‚¬ìš©í•  MIME íƒ€ì…:', mimeType)
      
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: mimeType
      })
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }
      
      mediaRecorder.onstop = async () => {
        console.log('ğŸ¤ Whisper STT ë…¹ìŒ ì™„ë£Œ')
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType })
        audioChunksRef.current = []
        
        console.log('ğŸ¤ ìƒì„±ëœ ì˜¤ë””ì˜¤ Blob:', {
          size: audioBlob.size,
          type: audioBlob.type
        })
        
        if (audioBlob.size > 0) {
          await processWhisperSTT(audioBlob)
        }
      }
      
      mediaRecorderRef.current = mediaRecorder
      console.log('ğŸ¤ MediaRecorder ì„¤ì • ì™„ë£Œ')
      
    } catch (error) {
      console.error('âŒ MediaRecorder ì„¤ì • ì‹¤íŒ¨:', error)
    }
  }

  // Whisper STT ì²˜ë¦¬
  const processWhisperSTT = async (audioBlob: Blob) => {
    try {
      setIsProcessing(true)
      console.log('ğŸ¤ Whisper STT ì²˜ë¦¬ ì‹œì‘')
      
      // íŒŒì¼ í™•ì¥ì ê²°ì •
      const fileExtension = audioBlob.type.includes('wav') ? 'wav' : 'webm'
      const fileName = `recording.${fileExtension}`
      
      const formData = new FormData()
      formData.append('audio', audioBlob, fileName)
      formData.append('modelId', 'openai-whisper')
      formData.append('language', selectedLanguage === 'ko-KR' ? 'ko' : 'en')
      
      console.log('ğŸ¤ STT ìš”ì²­ ë°ì´í„°:', {
        fileName,
        blobType: audioBlob.type,
        blobSize: audioBlob.size,
        modelId: 'openai-whisper',
        language: selectedLanguage === 'ko-KR' ? 'ko' : 'en'
      })
      
      const response = await fetch('/api/stt', {
        method: 'POST',
        body: formData,
      })
      
      if (!response.ok) {
        const errorData = await response.text()
        console.error('âŒ STT API ì‘ë‹µ ì˜¤ë¥˜:', {
          status: response.status,
          statusText: response.statusText,
          errorData
        })
        throw new Error(`STT ì²˜ë¦¬ ì‹¤íŒ¨: ${response.status} ${response.statusText}`)
      }
      
      const result = await response.json()
      console.log('âœ… Whisper STT ê²°ê³¼:', result)
      
      // STT API ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (transcript ë˜ëŠ” text í•„ë“œ)
      const transcriptText = result.transcript || result.text || ''
      console.log('ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:', transcriptText)
      
      if (transcriptText && transcriptText.trim()) {
        handleUserSpeech(transcriptText.trim())
      } else {
        console.warn('âš ï¸ ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤:', result)
      }
      
    } catch (error) {
      console.error('âŒ Whisper STT ì˜¤ë¥˜:', error)
    } finally {
      setIsProcessing(false)
    }
  }

  // ì‚¬ìš©ì ìŒì„± ì²˜ë¦¬
  const handleUserSpeech = async (transcript: string) => {
    if (!transcript.trim()) return

    // ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    const userMessage: Message = {
      role: 'user',
      content: transcript,
      timestamp: new Date()
    }
    setConversation(prev => [...prev, userMessage])

    // AI ì‘ë‹µ ìƒì„± ì‹œì‘
    setIsProcessing(true)
    await generateStreamingResponse(transcript)
  }

  // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° AI ì‘ë‹µ ìƒì„±
  const generateStreamingResponse = async (userInput: string) => {
    try {
      console.log('ğŸ¤– AI ì‘ë‹µ ìƒì„± ì‹œì‘')
      
      // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ë¹ˆ ë©”ì‹œì§€ ì¶”ê°€
      const assistantMessage: Message = {
        role: 'assistant',
        content: '',
        timestamp: new Date(),
        isStreaming: true
      }
      setConversation(prev => [...prev, assistantMessage])

      const response = await fetch('/api/chat-stream', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          message: userInput,
          conversation: conversation.slice(-10) // ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ì „ì†¡
        }),
      })

      if (!response.ok) {
        throw new Error('ì‘ë‹µ ìƒì„± ì‹¤íŒ¨')
      }

      const reader = response.body?.getReader()
      const decoder = new TextDecoder()
      let fullResponse = ''

      if (reader) {
        while (true) {
          const { done, value } = await reader.read()
          if (done) break

          const chunk = decoder.decode(value)
          const lines = chunk.split('\n')

          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = line.slice(6)
              if (data === '[DONE]') {
                break
              }
              
              try {
                const parsed = JSON.parse(data)
                const content = parsed.choices?.[0]?.delta?.content || ''
                if (content) {
                  fullResponse += content
                  
                  // ì‹¤ì‹œê°„ìœ¼ë¡œ ëŒ€í™” ì—…ë°ì´íŠ¸
                  setConversation(prev => {
                    const newConv = [...prev]
                    const lastMessage = newConv[newConv.length - 1]
                    if (lastMessage && lastMessage.role === 'assistant') {
                      lastMessage.content = fullResponse
                    }
                    return newConv
                  })
                }
              } catch (e) {
                // JSON íŒŒì‹± ì˜¤ë¥˜ ë¬´ì‹œ
              }
            }
          }
        }
      }

      // ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
      setConversation(prev => {
        const newConv = [...prev]
        const lastMessage = newConv[newConv.length - 1]
        if (lastMessage && lastMessage.role === 'assistant') {
          lastMessage.isStreaming = false
        }
        return newConv
      })

      setIsProcessing(false)

      // ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì¦‰ì‹œ TTS ì‹œì‘
      if (fullResponse.trim()) {
        console.log('ğŸ”Š ì „ì²´ ì‘ë‹µ TTS ì‹œì‘:', fullResponse.trim())
        speakTextImmediate(fullResponse.trim())
      }

    } catch (error) {
      console.error('âŒ AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨:', error)
      setIsProcessing(false)
      
      const errorMessage: Message = {
        role: 'assistant',
        content: 'ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        timestamp: new Date()
      }
      setConversation(prev => [...prev, errorMessage])
    }
  }

  // OpenAI TTS ì²˜ë¦¬
  const speakWithOpenAI = async (text: string) => {
    try {
      console.log('ğŸ”Š OpenAI TTS ìš”ì²­ ì‹œì‘')
      setIsSpeaking(true)

      const response = await fetch('/api/tts', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          text: text,
          voice: 'alloy', // OpenAI TTS ìŒì„± (alloy, echo, fable, onyx, nova, shimmer)
          speed: 1.2
        }),
      })

      if (!response.ok) {
        throw new Error('TTS ìƒì„± ì‹¤íŒ¨')
      }

      const audioBlob = await response.blob()
      console.log('ğŸ”Š OpenAI TTS ì˜¤ë””ì˜¤ ìƒì„± ì™„ë£Œ:', audioBlob.size)

      // ì˜¤ë””ì˜¤ ì¬ìƒ
      const audioUrl = URL.createObjectURL(audioBlob)
      const audio = new Audio(audioUrl)
      currentAudioRef.current = audio
      
      audio.onended = () => {
        console.log('âœ… OpenAI TTS ì¬ìƒ ì™„ë£Œ')
        setIsSpeaking(false)
        URL.revokeObjectURL(audioUrl)
        currentAudioRef.current = null
        
        // TTS ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ë‹¤ì‹œ ë“£ê¸° ì‹œì‘
        setTimeout(() => {
          if (useWhisperSTT) {
            // Whisper STTëŠ” ìˆ˜ë™ìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•¨
            console.log('ğŸ”„ Whisper STT ì¬ì‹œì‘ ëŒ€ê¸° ì¤‘ (ìˆ˜ë™ ì‹œì‘ í•„ìš”)')
          } else {
            // Web Speech APIëŠ” ìë™ ì¬ì‹œì‘
            startListening()
          }
        }, 300)
      }

      audio.onerror = (error) => {
        console.error('âŒ OpenAI TTS ì¬ìƒ ì˜¤ë¥˜:', error)
        setIsSpeaking(false)
        URL.revokeObjectURL(audioUrl)
        currentAudioRef.current = null
      }

      await audio.play()

    } catch (error) {
      console.error('âŒ OpenAI TTS ì˜¤ë¥˜:', error)
      setIsSpeaking(false)
    }
  }

  // ì¦‰ì‹œ TTS (OpenAI TTS ì‚¬ìš©)
  const speakTextImmediate = (text: string) => {
    if (!text.trim()) return
    speakWithOpenAI(text)
  }

  // TTSë¡œ í…ìŠ¤íŠ¸ ì½ê¸° (OpenAI TTS ì‚¬ìš©)
  const speakText = async (text: string) => {
    if (!text.trim()) return
    await speakWithOpenAI(text)
  }

  // ìŒì„± ì¸ì‹ ì‹œì‘ (Web Speech API ë˜ëŠ” Whisper STT)
  const startListening = async () => {
    if (useWhisperSTT) {
      // Whisper STT ì‚¬ìš©
      if (!mediaRecorderRef.current) {
        await setupMediaRecorder()
      }
      
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'inactive') {
        try {
          audioChunksRef.current = []
          mediaRecorderRef.current.start()
          setIsListening(true)
          console.log('ğŸ¤ Whisper STT ë…¹ìŒ ì‹œì‘')
          
          // 3ì´ˆ í›„ ìë™ ì¤‘ì§€ (ì¡°ì • ê°€ëŠ¥)
          setTimeout(() => {
            if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
              mediaRecorderRef.current.stop()
              setIsListening(false)
            }
          }, 3000)
          
        } catch (error) {
          console.error('âŒ Whisper STT ì‹œì‘ ì‹¤íŒ¨:', error)
        }
      }
    } else {
      // Web Speech API ì‚¬ìš©
      if (recognitionRef.current && !isListening) {
        try {
          recognitionRef.current.start()
        } catch (error) {
          console.error('âŒ ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:', error)
        }
      }
    }
  }

  // ìŒì„± ì¸ì‹ ì¤‘ì§€
  const stopListening = () => {
    if (useWhisperSTT) {
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
        mediaRecorderRef.current.stop()
        setIsListening(false)
      }
    } else {
      if (recognitionRef.current && isListening) {
        recognitionRef.current.stop()
      }
    }
  }

  // ì—°ì† ëŒ€í™” ì‹œì‘/ì¤‘ì§€
  const toggleContinuousChat = () => {
    if (isListening) {
      stopListening()
    } else {
      startListening()
    }
  }

  // TTS ì¤‘ì§€
  const stopSpeaking = () => {
    if (currentAudioRef.current) {
      currentAudioRef.current.pause()
      currentAudioRef.current.currentTime = 0
      currentAudioRef.current = null
    }
    setIsSpeaking(false)
  }

  return (
    <div className="max-w-4xl mx-auto p-6">
      <div className="bg-white rounded-lg shadow-lg p-6">
        <div className="text-center mb-8">
          <h2 className="text-2xl font-bold text-gray-900 mb-2">
            ìŠ¤íŠ¸ë¦¬ë° ìŒì„± ëŒ€í™”
          </h2>
          <p className="text-gray-600">
            {useWhisperSTT 
              ? 'OpenAI Whisper STT + OpenAI TTSë¡œ ê³ í’ˆì§ˆ ìŒì„± ëŒ€í™”ë¥¼ ê²½í—˜í•´ë³´ì„¸ìš”' 
              : 'Web Speech API + OpenAI TTSë¡œ ë¹ ë¥´ê³  ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ëŒ€í™”ë¥¼ ê²½í—˜í•´ë³´ì„¸ìš”'
            }
          </p>
          {isWhale && (
            <div className="mt-2 p-2 bg-blue-50 rounded-lg border border-blue-200">
              <p className="text-sm text-blue-800">
                ğŸ‹ Whale ë¸Œë¼ìš°ì €ê°€ ê°ì§€ë˜ì–´ OpenAI Whisper STTë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
              </p>
            </div>
          )}
        </div>

        {/* ì–¸ì–´ ì„ íƒ */}
        <div className="flex justify-center mb-6">
          <select
            value={selectedLanguage}
            onChange={(e) => setSelectedLanguage(e.target.value)}
            disabled={isListening}
            className="px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-blue-500 focus:border-transparent"
          >
            <option value="ko-KR">í•œêµ­ì–´</option>
            <option value="en-US">English (US)</option>
            <option value="ja-JP">æ—¥æœ¬èª</option>
            <option value="zh-CN">ä¸­æ–‡</option>
          </select>
        </div>

        {/* ì»¨íŠ¸ë¡¤ ë²„íŠ¼ */}
        <div className="flex justify-center space-x-4 mb-8">
          <button
            onClick={toggleContinuousChat}
            disabled={isProcessing}
            className={`flex items-center space-x-2 px-6 py-3 rounded-lg font-medium transition-colors ${
              isListening
                ? 'bg-red-500 text-white hover:bg-red-600'
                : 'bg-blue-500 text-white hover:bg-blue-600 disabled:bg-gray-300'
            }`}
          >
            {isListening ? <MicOff size={20} /> : <Mic size={20} />}
            <span>
              {isListening ? 'ë“£ê¸° ì¤‘ì§€' : 'ëŒ€í™” ì‹œì‘'}
            </span>
          </button>

          {isSpeaking && (
            <button
              onClick={stopSpeaking}
              className="flex items-center space-x-2 px-6 py-3 bg-orange-500 text-white rounded-lg hover:bg-orange-600 font-medium"
            >
              <VolumeX size={20} />
              <span>ìŒì„± ì¤‘ì§€</span>
            </button>
          )}
        </div>

        {/* ìƒíƒœ í‘œì‹œ */}
        <div className="flex justify-center space-x-6 mb-8">
          <div className={`flex items-center space-x-2 ${
            isListening ? 'text-green-600' : 'text-gray-400'
          }`}>
            <Mic size={20} />
            <span className="font-medium">
              {isListening ? 'ë“£ê³  ìˆìŒ' : 'ëŒ€ê¸° ì¤‘'}
            </span>
          </div>
          
          <div className={`flex items-center space-x-2 ${
            isProcessing ? 'text-blue-600' : 'text-gray-400'
          }`}>
            <div className={`w-3 h-3 rounded-full ${
              isProcessing ? 'bg-blue-500 animate-pulse' : 'bg-gray-300'
            }`} />
            <span className="font-medium">
              {isProcessing ? 'AI ìƒê° ì¤‘' : 'ëŒ€ê¸° ì¤‘'}
            </span>
          </div>

          <div className={`flex items-center space-x-2 ${
            isSpeaking ? 'text-purple-600' : 'text-gray-400'
          }`}>
            <Volume2 size={20} />
            <span className="font-medium">
              {isSpeaking ? 'ë§í•˜ëŠ” ì¤‘' : 'ëŒ€ê¸° ì¤‘'}
            </span>
          </div>
        </div>

        {/* í˜„ì¬ ì¸ì‹ ì¤‘ì¸ í…ìŠ¤íŠ¸ */}
        {currentTranscript && (
          <div className="mb-4 p-3 bg-blue-50 rounded-lg border-l-4 border-blue-400">
            <p className="text-blue-800 text-sm">
              ì¸ì‹ ì¤‘: {currentTranscript}
            </p>
          </div>
        )}

        {/* ëŒ€í™” ê¸°ë¡ */}
        <div className="bg-gray-50 rounded-lg p-4 max-h-96 overflow-y-auto">
          {conversation.length === 0 ? (
            <p className="text-gray-500 text-center">
              "ëŒ€í™” ì‹œì‘" ë²„íŠ¼ì„ í´ë¦­í•˜ê³  ë§í•´ë³´ì„¸ìš”! ğŸ¤
            </p>
          ) : (
            <div className="space-y-4">
              {conversation.map((message, index) => (
                <div
                  key={index}
                  className={`flex ${
                    message.role === 'user' ? 'justify-end' : 'justify-start'
                  }`}
                >
                  <div
                    className={`max-w-xs lg:max-w-md px-4 py-2 rounded-lg ${
                      message.role === 'user'
                        ? 'bg-blue-500 text-white'
                        : 'bg-white text-gray-900 border'
                    }`}
                  >
                    <p className="text-sm">
                      {message.content}
                      {message.isStreaming && (
                        <span className="inline-block w-2 h-4 bg-gray-400 animate-pulse ml-1" />
                      )}
                    </p>
                    <p className={`text-xs mt-1 ${
                      message.role === 'user' ? 'text-blue-100' : 'text-gray-500'
                    }`}>
                      {message.timestamp.toLocaleTimeString()}
                    </p>
                  </div>
                </div>
              ))}
            </div>
          )}
        </div>

        {/* ì‚¬ìš©ë²• ì•ˆë‚´ */}
        <div className="mt-6 p-4 bg-green-50 rounded-lg">
          <h3 className="font-medium text-green-900 mb-2">íŠ¹ì§•</h3>
          <ul className="text-sm text-green-800 space-y-1">
            <li>â€¢ {useWhisperSTT ? 'OpenAI Whisper STT ì‚¬ìš© (ì •í™•í•œ ì¸ì‹)' : 'ë¸Œë¼ìš°ì € ë‚´ì¥ ìŒì„± ì¸ì‹ ì‚¬ìš© (ë¹ ë¥¸ ì‘ë‹µ)'}</li>
            <li>â€¢ AI ì‘ë‹µì´ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°ë©ë‹ˆë‹¤</li>
            <li>â€¢ OpenAI TTSë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ì‘ë‹µ</li>
            <li>â€¢ AI ì‘ë‹µ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ë‹¤ì‹œ ë“£ê¸° ì‹œì‘</li>
            <li>â€¢ ì—°ì† ëŒ€í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤</li>
            {isWhale && <li>â€¢ ğŸ‹ Whale ë¸Œë¼ìš°ì € ìµœì í™” ì§€ì›</li>}
          </ul>
        </div>
      </div>
    </div>
  )
}
```

### `src/app/voice-chat/page.tsx`
```tsx
import { StreamingVoiceChat } from '@/components/StreamingVoiceChat'

export default function VoiceChatPage() {
  return (
    <div className="min-h-screen bg-gray-100">
      <StreamingVoiceChat />
    </div>
  )
}
```

---

## API ë¼ìš°íŠ¸

### `src/app/api/chat-stream/route.ts`
```typescript
import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'

const openai = new OpenAI({
  apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,
})

export async function POST(request: NextRequest) {
  try {
    const { message, conversation = [] } = await request.json()

    if (!message) {
      return NextResponse.json(
        { error: 'ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.' },
        { status: 400 }
      )
    }

    // ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
    const messages = [
      {
        role: 'system' as const,
        content: 'ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.'
      },
      ...conversation.slice(-8).map((msg: any) => ({
        role: msg.role,
        content: msg.content
      })),
      {
        role: 'user' as const,
        content: message
      }
    ]

    console.log('ğŸ¤– ChatGPT ìš”ì²­:', {
      messageCount: messages.length,
      lastMessage: message
    })

    // OpenAI ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­
    const stream = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: messages,
      max_tokens: 50,
      temperature: 0.3,
      presence_penalty: 0.1,
      frequency_penalty: 0.1,
      stream: true,
    })

    // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
    const encoder = new TextEncoder()
    
    const readableStream = new ReadableStream({
      async start(controller) {
        try {
          for await (const chunk of stream) {
            const content = chunk.choices[0]?.delta?.content
            if (content) {
              const data = `data: ${JSON.stringify(chunk)}\n\n`
              controller.enqueue(encoder.encode(data))
            }
          }
          
          // ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
          controller.enqueue(encoder.encode('data: [DONE]\n\n'))
          controller.close()
        } catch (error) {
          console.error('ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜:', error)
          controller.error(error)
        }
      }
    })

    return new Response(readableStream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    })

  } catch (error) {
    console.error('ChatGPT API ì˜¤ë¥˜:', error)
    return NextResponse.json(
      { error: 'AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' },
      { status: 500 }
    )
  }
}
```

### `src/app/api/tts/route.ts`
```typescript
import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'

const openai = new OpenAI({
  apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,
})

export async function POST(request: NextRequest) {
  try {
    const { text, voice = 'alloy', speed = 1.0 } = await request.json()

    if (!text) {
      return NextResponse.json(
        { error: 'í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.' },
        { status: 400 }
      )
    }

    console.log('ğŸ”Š OpenAI TTS ìš”ì²­:', {
      text: text.substring(0, 50) + (text.length > 50 ? '...' : ''),
      voice,
      speed
    })

    // OpenAI TTS API í˜¸ì¶œ
    const mp3 = await openai.audio.speech.create({
      model: 'tts-1',
      voice: voice as 'alloy' | 'echo' | 'fable' | 'onyx' | 'nova' | 'shimmer',
      input: text,
      speed: speed,
    })

    // ArrayBufferë¥¼ Bufferë¡œ ë³€í™˜
    const buffer = Buffer.from(await mp3.arrayBuffer())

    console.log('âœ… OpenAI TTS ìƒì„± ì™„ë£Œ:', {
      bufferSize: buffer.length,
      voice,
      speed
    })

    return new Response(buffer, {
      headers: {
        'Content-Type': 'audio/mpeg',
        'Content-Length': buffer.length.toString(),
      },
    })

  } catch (error) {
    console.error('TTS API ì˜¤ë¥˜:', error)
    return NextResponse.json(
      { error: 'TTS ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' },
      { status: 500 }
    )
  }
}
```

### `src/app/api/stt/route.ts` (Whale ë¸Œë¼ìš°ì €ìš© Whisper STT)
```typescript
import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'

const openai = new OpenAI({
  apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,
})

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData()
    const file = formData.get('audio') as File
    const modelId = formData.get('modelId') as string
    const language = formData.get('language') as string || 'ko'

    if (!file || !modelId) {
      return NextResponse.json(
        { error: 'ì˜¤ë””ì˜¤ íŒŒì¼ê³¼ ëª¨ë¸ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.' },
        { status: 400 }
      )
    }

    // OpenAI Whisperë§Œ ì²˜ë¦¬ (ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™”ìš©)
    if (modelId !== 'openai-whisper') {
      return NextResponse.json(
        { error: 'ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™”ëŠ” OpenAI Whisperë§Œ ì§€ì›í•©ë‹ˆë‹¤.' },
        { status: 400 }
      )
    }

    // íŒŒì¼ í¬ê¸° ê²€ì¦ (10MB ì œí•œ - ì‹¤ì‹œê°„ìš©)
    if (file.size > 10 * 1024 * 1024) {
      return NextResponse.json(
        { error: 'íŒŒì¼ í¬ê¸°ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.' },
        { status: 400 }
      )
    }

    console.log('ğŸ¤ Whisper STT ì²˜ë¦¬:', {
      fileName: file.name,
      fileSize: file.size,
      language
    })

    // OpenAI Whisper API í˜¸ì¶œ
    const transcription = await openai.audio.transcriptions.create({
      file: file,
      model: 'whisper-1',
      language: language === 'ko' ? 'ko' : 'en',
      response_format: 'json',
    })

    console.log('âœ… Whisper STT ì™„ë£Œ:', transcription.text)

    return NextResponse.json({
      modelId: 'openai-whisper',
      modelName: 'OpenAI Whisper',
      text: transcription.text,
      confidence: 0.95, // WhisperëŠ” confidence ì ìˆ˜ë¥¼ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’
      language,
      timestamp: new Date().toISOString(),
      fileName: file.name,
      fileSize: file.size
    })

  } catch (error) {
    console.error('Whisper STT ì˜¤ë¥˜:', error)
    return NextResponse.json(
      { error: 'Whisper STT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' },
      { status: 500 }
    )
  }
}
```

---

## í™˜ê²½ ì„¤ì •

### `.env.local`
```env
# OpenAI API Key (í•„ìˆ˜ - STT, TTS, ChatGPT ëª¨ë‘ ì‚¬ìš©)
NEXT_PUBLIC_OPENAI_API_KEY=your_openai_api_key_here
```

### `next.config.js`
```javascript
/** @type {import('next').NextConfig} */
const nextConfig = {
  // ì›¹íŒ© ì„¤ì •: Node.js ëª¨ë“ˆì„ ë¸Œë¼ìš°ì €ì—ì„œ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì„¤ì •
  webpack: (config, { isServer }) => {
    if (!isServer) {
      config.resolve.fallback = {
        ...config.resolve.fallback,
        fs: false,
        path: false,
        os: false,
        crypto: false,
        stream: false,
        buffer: false,
      }
    }
    return config
  },
}

module.exports = nextConfig
```

---

## íŒ¨í‚¤ì§€ ì„¤ì •

### `package.json`
```json
{
  "name": "realtime-voice-chat",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev -p 3001",
    "build": "next build",
    "start": "next start -p 3001",
    "lint": "next lint"
  },
  "dependencies": {
    "next": "14.0.4",
    "react": "^18",
    "react-dom": "^18",
    "typescript": "^5",
    "openai": "^4.20.1",
    "lucide-react": "^0.294.0",
    "tailwindcss": "^3.3.0"
  },
  "devDependencies": {
    "@types/node": "^20",
    "eslint": "^8",
    "eslint-config-next": "14.0.4"
  }
}
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

1. **í™˜ê²½ ì„¤ì •**
   ```bash
   # OpenAI API í‚¤ë¥¼ .env.localì— ì„¤ì •
   echo "NEXT_PUBLIC_OPENAI_API_KEY=your_api_key" > .env.local
   ```

2. **íŒ¨í‚¤ì§€ ì„¤ì¹˜**
   ```bash
   npm install
   ```

3. **ê°œë°œ ì„œë²„ ì‹¤í–‰**
   ```bash
   npm run dev
   ```

4. **ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†**
   ```
   http://localhost:3001/voice-chat
   ```

---

## ğŸ¯ ì£¼ìš” íŠ¹ì§•

- **ğŸ‹ ë¸Œë¼ìš°ì € ìë™ ê°ì§€**: Whale ë¸Œë¼ìš°ì €ì—ì„œ ìë™ìœ¼ë¡œ OpenAI Whisper STT ì‚¬ìš©
- **ğŸ¤ ì´ì¤‘ STT ì‹œìŠ¤í…œ**: Web Speech API (ë¹ ë¦„) + OpenAI Whisper (ì •í™•í•¨)
- **ğŸ”Š OpenAI TTS**: ëª¨ë“  ë¸Œë¼ìš°ì €ì—ì„œ ê³ í’ˆì§ˆ ìŒì„± í•©ì„±
- **âš¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: AI ì‘ë‹µì´ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°
- **ğŸ”„ ìë™ ì¬ì‹œì‘**: TTS ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ìŒì„± ì¸ì‹ ì¬ì‹œì‘
- **ğŸŒ ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´

---

## ğŸ“ ì‚¬ìš©ë²•

1. "ëŒ€í™” ì‹œì‘" ë²„íŠ¼ í´ë¦­
2. ë§ˆì´í¬ì— ëŒ€ê³  ë§í•˜ê¸°
3. AI ì‘ë‹µì„ ìŒì„±ìœ¼ë¡œ ë“£ê¸°
4. ìë™ìœ¼ë¡œ ë‹¤ìŒ ìŒì„± ì¸ì‹ ì‹œì‘
5. ì—°ì† ëŒ€í™” ì§„í–‰!
