# 실시간 음성 대화 시스템 전체 코드

## 📋 목차
1. [프론트엔드 컴포넌트](#프론트엔드-컴포넌트)
2. [API 라우트](#api-라우트)
3. [환경 설정](#환경-설정)
4. [패키지 설정](#패키지-설정)

---

## 프론트엔드 컴포넌트

### `src/components/StreamingVoiceChat.tsx`
```tsx
'use client'

import React, { useState, useRef, useEffect } from 'react'
import { Mic, MicOff, Volume2, VolumeX, Play, Pause } from 'lucide-react'

interface StreamingVoiceChatProps {}

interface Message {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  isStreaming?: boolean
}

export const StreamingVoiceChat: React.FC<StreamingVoiceChatProps> = () => {
  const [isListening, setIsListening] = useState(false)
  const [isProcessing, setIsProcessing] = useState(false)
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [conversation, setConversation] = useState<Message[]>([])
  const [currentTranscript, setCurrentTranscript] = useState('')
  const [streamingResponse, setStreamingResponse] = useState('')
  const [selectedLanguage, setSelectedLanguage] = useState('ko-KR')
  const [isWhale, setIsWhale] = useState(false)
  const [useWhisperSTT, setUseWhisperSTT] = useState(false)

  const recognitionRef = useRef<any>(null)
  const synthRef = useRef<SpeechSynthesis | null>(null)
  const streamingTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const currentAudioRef = useRef<HTMLAudioElement | null>(null)

  // 브라우저 감지 함수
  const detectBrowser = () => {
    if (typeof window === 'undefined') return false
    
    const userAgent = navigator.userAgent.toLowerCase()
    const isWhaleBrowser = userAgent.includes('whale')
    
    console.log('🐋 브라우저 감지:', {
      userAgent: navigator.userAgent,
      isWhale: isWhaleBrowser,
      webSpeechSupport: !!(window as any).SpeechRecognition || !!(window as any).webkitSpeechRecognition
    })
    
    return isWhaleBrowser
  }

  // 브라우저 감지 및 STT 방식 결정
  useEffect(() => {
    if (typeof window !== 'undefined') {
      const whaleBrowser = detectBrowser()
      setIsWhale(whaleBrowser)
      setUseWhisperSTT(whaleBrowser) // Whale 브라우저면 자동으로 Whisper STT 사용
      
      console.log('🌐 브라우저 환경 확인:', {
        userAgent: navigator.userAgent,
        language: navigator.language,
        languages: navigator.languages,
        onLine: navigator.onLine,
        protocol: window.location.protocol,
        isWhale: whaleBrowser,
        useWhisperSTT: whaleBrowser
      })
    }
  }, [])

  // Web Speech API 설정 (Whale이 아닌 경우에만)
  useEffect(() => {
    if (typeof window !== 'undefined' && !useWhisperSTT) {
      const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
      
      console.log('🎙️ Speech Recognition 지원:', {
        SpeechRecognition: !!SpeechRecognition,
        webkitSpeechRecognition: !!(window as any).webkitSpeechRecognition,
        SpeechRecognitionNative: !!(window as any).SpeechRecognition
      })
      
      if (SpeechRecognition) {
        const recognition = new SpeechRecognition()
        
        // 기본 설정만 사용
        recognition.continuous = false  // 일단 false로 테스트
        recognition.interimResults = true
        recognition.lang = selectedLanguage
        recognition.maxAlternatives = 1  // 1로 줄여서 테스트
        
        console.log('🔧 음성 인식 설정:', {
          continuous: recognition.continuous,
          interimResults: recognition.interimResults,
          lang: recognition.lang,
          maxAlternatives: recognition.maxAlternatives
        })

        recognition.onstart = () => {
          console.log('🎤 음성 인식 시작')
          setIsListening(true)
        }

        recognition.onresult = (event: any) => {
          console.log('📝 음성 인식 결과 이벤트:', {
            resultIndex: event.resultIndex,
            resultsLength: event.results.length,
            results: Array.from(event.results).map((result: any, index: number) => ({
              index,
              transcript: result[0].transcript,
              confidence: result[0].confidence,
              isFinal: result.isFinal
            }))
          })

          let interimTranscript = ''
          let finalTranscript = ''

          for (let i = event.resultIndex; i < event.results.length; i++) {
            const result = event.results[i]
            
            // 모든 대안 확인
            let bestTranscript = ''
            let bestConfidence = 0
            
            for (let j = 0; j < result.length; j++) {
              const alternative = result[j]
              console.log(`📝 결과 ${i}-${j}:`, {
                transcript: alternative.transcript,
                confidence: alternative.confidence,
                isFinal: result.isFinal
              })
              
              if (alternative.confidence > bestConfidence || j === 0) {
                bestTranscript = alternative.transcript
                bestConfidence = alternative.confidence
              }
            }
            
            if (bestTranscript.trim()) { // 빈 문자열 체크
              if (result.isFinal) {
                finalTranscript += bestTranscript
              } else {
                interimTranscript += bestTranscript
              }
            }
          }

          if (interimTranscript) {
            console.log('🔄 중간 결과:', interimTranscript)
            setCurrentTranscript(interimTranscript)
          }

          if (finalTranscript) {
            console.log('✅ 최종 음성 인식:', finalTranscript)
            handleUserSpeech(finalTranscript)
            setCurrentTranscript('')
          }
        }

        recognition.onerror = (event: any) => {
          console.error('❌ 음성 인식 오류:', {
            error: event.error,
            message: event.message,
            type: event.type
          })
          setIsListening(false)
        }

        recognition.onend = () => {
          console.log('🔇 음성 인식 종료')
          setIsListening(false)
          
          // continuous가 false일 때 자동 재시작
          if (!recognition.continuous && isListening) {
            console.log('🔄 음성 인식 자동 재시작')
            setTimeout(() => {
              if (recognitionRef.current && !isProcessing && !isSpeaking) {
                try {
                  recognitionRef.current.start()
                } catch (error) {
                  console.error('❌ 재시작 실패:', error)
                }
              }
            }, 100)
          }
        }

        recognition.onaudiostart = () => {
          console.log('🎵 오디오 캡처 시작')
        }

        recognition.onaudioend = () => {
          console.log('🎵 오디오 캡처 종료')
        }

        recognition.onsoundstart = () => {
          console.log('🔊 소리 감지 시작')
        }

        recognition.onsoundend = () => {
          console.log('🔊 소리 감지 종료')
        }

        recognition.onspeechstart = () => {
          console.log('🗣️ 음성 감지 시작')
        }

        recognition.onspeechend = () => {
          console.log('🗣️ 음성 감지 종료')
        }

        recognition.onnomatch = () => {
          console.log('❓ 음성 인식 결과 없음')
        }

        recognitionRef.current = recognition
      }

      synthRef.current = window.speechSynthesis
    }
  }, [selectedLanguage, useWhisperSTT])

  // Whisper STT를 위한 MediaRecorder 설정
  const setupMediaRecorder = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        } 
      })
      
      // 지원되는 MIME 타입 확인 후 설정
      let mimeType = 'audio/webm;codecs=opus'
      if (MediaRecorder.isTypeSupported('audio/wav')) {
        mimeType = 'audio/wav'
      } else if (MediaRecorder.isTypeSupported('audio/webm')) {
        mimeType = 'audio/webm'
      }
      
      console.log('🎤 사용할 MIME 타입:', mimeType)
      
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: mimeType
      })
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }
      
      mediaRecorder.onstop = async () => {
        console.log('🎤 Whisper STT 녹음 완료')
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType })
        audioChunksRef.current = []
        
        console.log('🎤 생성된 오디오 Blob:', {
          size: audioBlob.size,
          type: audioBlob.type
        })
        
        if (audioBlob.size > 0) {
          await processWhisperSTT(audioBlob)
        }
      }
      
      mediaRecorderRef.current = mediaRecorder
      console.log('🎤 MediaRecorder 설정 완료')
      
    } catch (error) {
      console.error('❌ MediaRecorder 설정 실패:', error)
    }
  }

  // Whisper STT 처리
  const processWhisperSTT = async (audioBlob: Blob) => {
    try {
      setIsProcessing(true)
      console.log('🎤 Whisper STT 처리 시작')
      
      // 파일 확장자 결정
      const fileExtension = audioBlob.type.includes('wav') ? 'wav' : 'webm'
      const fileName = `recording.${fileExtension}`
      
      const formData = new FormData()
      formData.append('audio', audioBlob, fileName)
      formData.append('modelId', 'openai-whisper')
      formData.append('language', selectedLanguage === 'ko-KR' ? 'ko' : 'en')
      
      console.log('🎤 STT 요청 데이터:', {
        fileName,
        blobType: audioBlob.type,
        blobSize: audioBlob.size,
        modelId: 'openai-whisper',
        language: selectedLanguage === 'ko-KR' ? 'ko' : 'en'
      })
      
      const response = await fetch('/api/stt', {
        method: 'POST',
        body: formData,
      })
      
      if (!response.ok) {
        const errorData = await response.text()
        console.error('❌ STT API 응답 오류:', {
          status: response.status,
          statusText: response.statusText,
          errorData
        })
        throw new Error(`STT 처리 실패: ${response.status} ${response.statusText}`)
      }
      
      const result = await response.json()
      console.log('✅ Whisper STT 결과:', result)
      
      // STT API 응답에서 텍스트 추출 (transcript 또는 text 필드)
      const transcriptText = result.transcript || result.text || ''
      console.log('📝 추출된 텍스트:', transcriptText)
      
      if (transcriptText && transcriptText.trim()) {
        handleUserSpeech(transcriptText.trim())
      } else {
        console.warn('⚠️ 인식된 텍스트가 없습니다:', result)
      }
      
    } catch (error) {
      console.error('❌ Whisper STT 오류:', error)
    } finally {
      setIsProcessing(false)
    }
  }

  // 사용자 음성 처리
  const handleUserSpeech = async (transcript: string) => {
    if (!transcript.trim()) return

    // 사용자 메시지 추가
    const userMessage: Message = {
      role: 'user',
      content: transcript,
      timestamp: new Date()
    }
    setConversation(prev => [...prev, userMessage])

    // AI 응답 생성 시작
    setIsProcessing(true)
    await generateStreamingResponse(transcript)
  }

  // 실시간 스트리밍 AI 응답 생성
  const generateStreamingResponse = async (userInput: string) => {
    try {
      console.log('🤖 AI 응답 생성 시작')
      
      // 스트리밍 응답을 위한 빈 메시지 추가
      const assistantMessage: Message = {
        role: 'assistant',
        content: '',
        timestamp: new Date(),
        isStreaming: true
      }
      setConversation(prev => [...prev, assistantMessage])

      const response = await fetch('/api/chat-stream', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          message: userInput,
          conversation: conversation.slice(-10) // 최근 10개 메시지만 전송
        }),
      })

      if (!response.ok) {
        throw new Error('응답 생성 실패')
      }

      const reader = response.body?.getReader()
      const decoder = new TextDecoder()
      let fullResponse = ''

      if (reader) {
        while (true) {
          const { done, value } = await reader.read()
          if (done) break

          const chunk = decoder.decode(value)
          const lines = chunk.split('\n')

          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = line.slice(6)
              if (data === '[DONE]') {
                break
              }
              
              try {
                const parsed = JSON.parse(data)
                const content = parsed.choices?.[0]?.delta?.content || ''
                if (content) {
                  fullResponse += content
                  
                  // 실시간으로 대화 업데이트
                  setConversation(prev => {
                    const newConv = [...prev]
                    const lastMessage = newConv[newConv.length - 1]
                    if (lastMessage && lastMessage.role === 'assistant') {
                      lastMessage.content = fullResponse
                    }
                    return newConv
                  })
                }
              } catch (e) {
                // JSON 파싱 오류 무시
              }
            }
          }
        }
      }

      // 스트리밍 완료
      setConversation(prev => {
        const newConv = [...prev]
        const lastMessage = newConv[newConv.length - 1]
        if (lastMessage && lastMessage.role === 'assistant') {
          lastMessage.isStreaming = false
        }
        return newConv
      })

      setIsProcessing(false)

      // 스트리밍 완료 후 즉시 TTS 시작
      if (fullResponse.trim()) {
        console.log('🔊 전체 응답 TTS 시작:', fullResponse.trim())
        speakTextImmediate(fullResponse.trim())
      }

    } catch (error) {
      console.error('❌ AI 응답 생성 실패:', error)
      setIsProcessing(false)
      
      const errorMessage: Message = {
        role: 'assistant',
        content: '죄송합니다. 응답 생성 중 오류가 발생했습니다.',
        timestamp: new Date()
      }
      setConversation(prev => [...prev, errorMessage])
    }
  }

  // OpenAI TTS 처리
  const speakWithOpenAI = async (text: string) => {
    try {
      console.log('🔊 OpenAI TTS 요청 시작')
      setIsSpeaking(true)

      const response = await fetch('/api/tts', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          text: text,
          voice: 'alloy', // OpenAI TTS 음성 (alloy, echo, fable, onyx, nova, shimmer)
          speed: 1.2
        }),
      })

      if (!response.ok) {
        throw new Error('TTS 생성 실패')
      }

      const audioBlob = await response.blob()
      console.log('🔊 OpenAI TTS 오디오 생성 완료:', audioBlob.size)

      // 오디오 재생
      const audioUrl = URL.createObjectURL(audioBlob)
      const audio = new Audio(audioUrl)
      currentAudioRef.current = audio
      
      audio.onended = () => {
        console.log('✅ OpenAI TTS 재생 완료')
        setIsSpeaking(false)
        URL.revokeObjectURL(audioUrl)
        currentAudioRef.current = null
        
        // TTS 완료 후 자동으로 다시 듣기 시작
        setTimeout(() => {
          if (useWhisperSTT) {
            // Whisper STT는 수동으로 시작해야 함
            console.log('🔄 Whisper STT 재시작 대기 중 (수동 시작 필요)')
          } else {
            // Web Speech API는 자동 재시작
            startListening()
          }
        }, 300)
      }

      audio.onerror = (error) => {
        console.error('❌ OpenAI TTS 재생 오류:', error)
        setIsSpeaking(false)
        URL.revokeObjectURL(audioUrl)
        currentAudioRef.current = null
      }

      await audio.play()

    } catch (error) {
      console.error('❌ OpenAI TTS 오류:', error)
      setIsSpeaking(false)
    }
  }

  // 즉시 TTS (OpenAI TTS 사용)
  const speakTextImmediate = (text: string) => {
    if (!text.trim()) return
    speakWithOpenAI(text)
  }

  // TTS로 텍스트 읽기 (OpenAI TTS 사용)
  const speakText = async (text: string) => {
    if (!text.trim()) return
    await speakWithOpenAI(text)
  }

  // 음성 인식 시작 (Web Speech API 또는 Whisper STT)
  const startListening = async () => {
    if (useWhisperSTT) {
      // Whisper STT 사용
      if (!mediaRecorderRef.current) {
        await setupMediaRecorder()
      }
      
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'inactive') {
        try {
          audioChunksRef.current = []
          mediaRecorderRef.current.start()
          setIsListening(true)
          console.log('🎤 Whisper STT 녹음 시작')
          
          // 3초 후 자동 중지 (조정 가능)
          setTimeout(() => {
            if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
              mediaRecorderRef.current.stop()
              setIsListening(false)
            }
          }, 3000)
          
        } catch (error) {
          console.error('❌ Whisper STT 시작 실패:', error)
        }
      }
    } else {
      // Web Speech API 사용
      if (recognitionRef.current && !isListening) {
        try {
          recognitionRef.current.start()
        } catch (error) {
          console.error('❌ 음성 인식 시작 실패:', error)
        }
      }
    }
  }

  // 음성 인식 중지
  const stopListening = () => {
    if (useWhisperSTT) {
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
        mediaRecorderRef.current.stop()
        setIsListening(false)
      }
    } else {
      if (recognitionRef.current && isListening) {
        recognitionRef.current.stop()
      }
    }
  }

  // 연속 대화 시작/중지
  const toggleContinuousChat = () => {
    if (isListening) {
      stopListening()
    } else {
      startListening()
    }
  }

  // TTS 중지
  const stopSpeaking = () => {
    if (currentAudioRef.current) {
      currentAudioRef.current.pause()
      currentAudioRef.current.currentTime = 0
      currentAudioRef.current = null
    }
    setIsSpeaking(false)
  }

  return (
    <div className="max-w-4xl mx-auto p-6">
      <div className="bg-white rounded-lg shadow-lg p-6">
        <div className="text-center mb-8">
          <h2 className="text-2xl font-bold text-gray-900 mb-2">
            스트리밍 음성 대화
          </h2>
          <p className="text-gray-600">
            {useWhisperSTT 
              ? 'OpenAI Whisper STT + OpenAI TTS로 고품질 음성 대화를 경험해보세요' 
              : 'Web Speech API + OpenAI TTS로 빠르고 자연스러운 음성 대화를 경험해보세요'
            }
          </p>
          {isWhale && (
            <div className="mt-2 p-2 bg-blue-50 rounded-lg border border-blue-200">
              <p className="text-sm text-blue-800">
                🐋 Whale 브라우저가 감지되어 OpenAI Whisper STT를 사용합니다
              </p>
            </div>
          )}
        </div>

        {/* 언어 선택 */}
        <div className="flex justify-center mb-6">
          <select
            value={selectedLanguage}
            onChange={(e) => setSelectedLanguage(e.target.value)}
            disabled={isListening}
            className="px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-blue-500 focus:border-transparent"
          >
            <option value="ko-KR">한국어</option>
            <option value="en-US">English (US)</option>
            <option value="ja-JP">日本語</option>
            <option value="zh-CN">中文</option>
          </select>
        </div>

        {/* 컨트롤 버튼 */}
        <div className="flex justify-center space-x-4 mb-8">
          <button
            onClick={toggleContinuousChat}
            disabled={isProcessing}
            className={`flex items-center space-x-2 px-6 py-3 rounded-lg font-medium transition-colors ${
              isListening
                ? 'bg-red-500 text-white hover:bg-red-600'
                : 'bg-blue-500 text-white hover:bg-blue-600 disabled:bg-gray-300'
            }`}
          >
            {isListening ? <MicOff size={20} /> : <Mic size={20} />}
            <span>
              {isListening ? '듣기 중지' : '대화 시작'}
            </span>
          </button>

          {isSpeaking && (
            <button
              onClick={stopSpeaking}
              className="flex items-center space-x-2 px-6 py-3 bg-orange-500 text-white rounded-lg hover:bg-orange-600 font-medium"
            >
              <VolumeX size={20} />
              <span>음성 중지</span>
            </button>
          )}
        </div>

        {/* 상태 표시 */}
        <div className="flex justify-center space-x-6 mb-8">
          <div className={`flex items-center space-x-2 ${
            isListening ? 'text-green-600' : 'text-gray-400'
          }`}>
            <Mic size={20} />
            <span className="font-medium">
              {isListening ? '듣고 있음' : '대기 중'}
            </span>
          </div>
          
          <div className={`flex items-center space-x-2 ${
            isProcessing ? 'text-blue-600' : 'text-gray-400'
          }`}>
            <div className={`w-3 h-3 rounded-full ${
              isProcessing ? 'bg-blue-500 animate-pulse' : 'bg-gray-300'
            }`} />
            <span className="font-medium">
              {isProcessing ? 'AI 생각 중' : '대기 중'}
            </span>
          </div>

          <div className={`flex items-center space-x-2 ${
            isSpeaking ? 'text-purple-600' : 'text-gray-400'
          }`}>
            <Volume2 size={20} />
            <span className="font-medium">
              {isSpeaking ? '말하는 중' : '대기 중'}
            </span>
          </div>
        </div>

        {/* 현재 인식 중인 텍스트 */}
        {currentTranscript && (
          <div className="mb-4 p-3 bg-blue-50 rounded-lg border-l-4 border-blue-400">
            <p className="text-blue-800 text-sm">
              인식 중: {currentTranscript}
            </p>
          </div>
        )}

        {/* 대화 기록 */}
        <div className="bg-gray-50 rounded-lg p-4 max-h-96 overflow-y-auto">
          {conversation.length === 0 ? (
            <p className="text-gray-500 text-center">
              "대화 시작" 버튼을 클릭하고 말해보세요! 🎤
            </p>
          ) : (
            <div className="space-y-4">
              {conversation.map((message, index) => (
                <div
                  key={index}
                  className={`flex ${
                    message.role === 'user' ? 'justify-end' : 'justify-start'
                  }`}
                >
                  <div
                    className={`max-w-xs lg:max-w-md px-4 py-2 rounded-lg ${
                      message.role === 'user'
                        ? 'bg-blue-500 text-white'
                        : 'bg-white text-gray-900 border'
                    }`}
                  >
                    <p className="text-sm">
                      {message.content}
                      {message.isStreaming && (
                        <span className="inline-block w-2 h-4 bg-gray-400 animate-pulse ml-1" />
                      )}
                    </p>
                    <p className={`text-xs mt-1 ${
                      message.role === 'user' ? 'text-blue-100' : 'text-gray-500'
                    }`}>
                      {message.timestamp.toLocaleTimeString()}
                    </p>
                  </div>
                </div>
              ))}
            </div>
          )}
        </div>

        {/* 사용법 안내 */}
        <div className="mt-6 p-4 bg-green-50 rounded-lg">
          <h3 className="font-medium text-green-900 mb-2">특징</h3>
          <ul className="text-sm text-green-800 space-y-1">
            <li>• {useWhisperSTT ? 'OpenAI Whisper STT 사용 (정확한 인식)' : '브라우저 내장 음성 인식 사용 (빠른 응답)'}</li>
            <li>• AI 응답이 실시간으로 스트리밍됩니다</li>
            <li>• OpenAI TTS로 자연스러운 음성 응답</li>
            <li>• AI 응답 완료 후 자동으로 다시 듣기 시작</li>
            <li>• 연속 대화가 가능합니다</li>
            {isWhale && <li>• 🐋 Whale 브라우저 최적화 지원</li>}
          </ul>
        </div>
      </div>
    </div>
  )
}
```

### `src/app/voice-chat/page.tsx`
```tsx
import { StreamingVoiceChat } from '@/components/StreamingVoiceChat'

export default function VoiceChatPage() {
  return (
    <div className="min-h-screen bg-gray-100">
      <StreamingVoiceChat />
    </div>
  )
}
```

---

## API 라우트

### `src/app/api/chat-stream/route.ts`
```typescript
import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'

const openai = new OpenAI({
  apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,
})

export async function POST(request: NextRequest) {
  try {
    const { message, conversation = [] } = await request.json()

    if (!message) {
      return NextResponse.json(
        { error: '메시지가 필요합니다.' },
        { status: 400 }
      )
    }

    // 대화 히스토리 구성
    const messages = [
      {
        role: 'system' as const,
        content: '당신은 친근하고 도움이 되는 AI 어시스턴트입니다. 간결하고 자연스럽게 대답해주세요.'
      },
      ...conversation.slice(-8).map((msg: any) => ({
        role: msg.role,
        content: msg.content
      })),
      {
        role: 'user' as const,
        content: message
      }
    ]

    console.log('🤖 ChatGPT 요청:', {
      messageCount: messages.length,
      lastMessage: message
    })

    // OpenAI 스트리밍 요청
    const stream = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: messages,
      max_tokens: 50,
      temperature: 0.3,
      presence_penalty: 0.1,
      frequency_penalty: 0.1,
      stream: true,
    })

    // 스트리밍 응답 생성
    const encoder = new TextEncoder()
    
    const readableStream = new ReadableStream({
      async start(controller) {
        try {
          for await (const chunk of stream) {
            const content = chunk.choices[0]?.delta?.content
            if (content) {
              const data = `data: ${JSON.stringify(chunk)}\n\n`
              controller.enqueue(encoder.encode(data))
            }
          }
          
          // 스트림 종료 신호
          controller.enqueue(encoder.encode('data: [DONE]\n\n'))
          controller.close()
        } catch (error) {
          console.error('스트리밍 오류:', error)
          controller.error(error)
        }
      }
    })

    return new Response(readableStream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    })

  } catch (error) {
    console.error('ChatGPT API 오류:', error)
    return NextResponse.json(
      { error: 'AI 응답 생성 중 오류가 발생했습니다.' },
      { status: 500 }
    )
  }
}
```

### `src/app/api/tts/route.ts`
```typescript
import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'

const openai = new OpenAI({
  apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,
})

export async function POST(request: NextRequest) {
  try {
    const { text, voice = 'alloy', speed = 1.0 } = await request.json()

    if (!text) {
      return NextResponse.json(
        { error: '텍스트가 필요합니다.' },
        { status: 400 }
      )
    }

    console.log('🔊 OpenAI TTS 요청:', {
      text: text.substring(0, 50) + (text.length > 50 ? '...' : ''),
      voice,
      speed
    })

    // OpenAI TTS API 호출
    const mp3 = await openai.audio.speech.create({
      model: 'tts-1',
      voice: voice as 'alloy' | 'echo' | 'fable' | 'onyx' | 'nova' | 'shimmer',
      input: text,
      speed: speed,
    })

    // ArrayBuffer를 Buffer로 변환
    const buffer = Buffer.from(await mp3.arrayBuffer())

    console.log('✅ OpenAI TTS 생성 완료:', {
      bufferSize: buffer.length,
      voice,
      speed
    })

    return new Response(buffer, {
      headers: {
        'Content-Type': 'audio/mpeg',
        'Content-Length': buffer.length.toString(),
      },
    })

  } catch (error) {
    console.error('TTS API 오류:', error)
    return NextResponse.json(
      { error: 'TTS 생성 중 오류가 발생했습니다.' },
      { status: 500 }
    )
  }
}
```

### `src/app/api/stt/route.ts` (Whale 브라우저용 Whisper STT)
```typescript
import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'

const openai = new OpenAI({
  apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,
})

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData()
    const file = formData.get('audio') as File
    const modelId = formData.get('modelId') as string
    const language = formData.get('language') as string || 'ko'

    if (!file || !modelId) {
      return NextResponse.json(
        { error: '오디오 파일과 모델 ID가 필요합니다.' },
        { status: 400 }
      )
    }

    // OpenAI Whisper만 처리 (실시간 음성 대화용)
    if (modelId !== 'openai-whisper') {
      return NextResponse.json(
        { error: '실시간 음성 대화는 OpenAI Whisper만 지원합니다.' },
        { status: 400 }
      )
    }

    // 파일 크기 검증 (10MB 제한 - 실시간용)
    if (file.size > 10 * 1024 * 1024) {
      return NextResponse.json(
        { error: '파일 크기가 10MB를 초과합니다.' },
        { status: 400 }
      )
    }

    console.log('🎤 Whisper STT 처리:', {
      fileName: file.name,
      fileSize: file.size,
      language
    })

    // OpenAI Whisper API 호출
    const transcription = await openai.audio.transcriptions.create({
      file: file,
      model: 'whisper-1',
      language: language === 'ko' ? 'ko' : 'en',
      response_format: 'json',
    })

    console.log('✅ Whisper STT 완료:', transcription.text)

    return NextResponse.json({
      modelId: 'openai-whisper',
      modelName: 'OpenAI Whisper',
      text: transcription.text,
      confidence: 0.95, // Whisper는 confidence 점수를 제공하지 않으므로 기본값
      language,
      timestamp: new Date().toISOString(),
      fileName: file.name,
      fileSize: file.size
    })

  } catch (error) {
    console.error('Whisper STT 오류:', error)
    return NextResponse.json(
      { error: 'Whisper STT 처리 중 오류가 발생했습니다.' },
      { status: 500 }
    )
  }
}
```

---

## 환경 설정

### `.env.local`
```env
# OpenAI API Key (필수 - STT, TTS, ChatGPT 모두 사용)
NEXT_PUBLIC_OPENAI_API_KEY=your_openai_api_key_here
```

### `next.config.js`
```javascript
/** @type {import('next').NextConfig} */
const nextConfig = {
  // 웹팩 설정: Node.js 모듈을 브라우저에서 사용하지 않도록 설정
  webpack: (config, { isServer }) => {
    if (!isServer) {
      config.resolve.fallback = {
        ...config.resolve.fallback,
        fs: false,
        path: false,
        os: false,
        crypto: false,
        stream: false,
        buffer: false,
      }
    }
    return config
  },
}

module.exports = nextConfig
```

---

## 패키지 설정

### `package.json`
```json
{
  "name": "realtime-voice-chat",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev -p 3001",
    "build": "next build",
    "start": "next start -p 3001",
    "lint": "next lint"
  },
  "dependencies": {
    "next": "14.0.4",
    "react": "^18",
    "react-dom": "^18",
    "typescript": "^5",
    "openai": "^4.20.1",
    "lucide-react": "^0.294.0",
    "tailwindcss": "^3.3.0"
  },
  "devDependencies": {
    "@types/node": "^20",
    "eslint": "^8",
    "eslint-config-next": "14.0.4"
  }
}
```

---

## 🚀 실행 방법

1. **환경 설정**
   ```bash
   # OpenAI API 키를 .env.local에 설정
   echo "NEXT_PUBLIC_OPENAI_API_KEY=your_api_key" > .env.local
   ```

2. **패키지 설치**
   ```bash
   npm install
   ```

3. **개발 서버 실행**
   ```bash
   npm run dev
   ```

4. **브라우저에서 접속**
   ```
   http://localhost:3001/voice-chat
   ```

---

## 🎯 주요 특징

- **🐋 브라우저 자동 감지**: Whale 브라우저에서 자동으로 OpenAI Whisper STT 사용
- **🎤 이중 STT 시스템**: Web Speech API (빠름) + OpenAI Whisper (정확함)
- **🔊 OpenAI TTS**: 모든 브라우저에서 고품질 음성 합성
- **⚡ 실시간 스트리밍**: AI 응답이 실시간으로 스트리밍
- **🔄 자동 재시작**: TTS 완료 후 자동으로 음성 인식 재시작
- **🌍 다국어 지원**: 한국어, 영어, 일본어, 중국어

---

## 📝 사용법

1. "대화 시작" 버튼 클릭
2. 마이크에 대고 말하기
3. AI 응답을 음성으로 듣기
4. 자동으로 다음 음성 인식 시작
5. 연속 대화 진행!
